{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbf080f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://113ec5fe70f9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fafe7f65820>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ac83d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.catalog.demo.jdbc.password', 'password'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///root/.ivy2/jars/org.apache.iceberg_iceberg-spark3-runtime-0.12.1.jar,file:///root/.ivy2/jars/software.amazon.awssdk_bundle-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.eventstream_eventstream-1.0.1.jar,file:///root/.ivy2/jars/software.amazon.awssdk_utils-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_annotations-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.2.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.28.jar,file:///root/.ivy2/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://113ec5fe70f9:40709/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,spark://113ec5fe70f9:40709/jars/org.slf4j_slf4j-api-1.7.28.jar,spark://113ec5fe70f9:40709/jars/software.amazon.awssdk_bundle-2.15.40.jar,spark://113ec5fe70f9:40709/jars/org.reactivestreams_reactive-streams-1.0.2.jar,spark://113ec5fe70f9:40709/jars/org.apache.iceberg_iceberg-spark3-runtime-0.12.1.jar,spark://113ec5fe70f9:40709/jars/software.amazon.awssdk_utils-2.15.40.jar,spark://113ec5fe70f9:40709/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar,spark://113ec5fe70f9:40709/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,spark://113ec5fe70f9:40709/jars/software.amazon.eventstream_eventstream-1.0.1.jar,spark://113ec5fe70f9:40709/jars/software.amazon.awssdk_annotations-2.15.40.jar'),\n",
       " ('spark.app.id', 'local-1637105399870'),\n",
       " ('spark.sql.catalog.demo.jdbc.user', 'admin'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/root/.ivy2/jars/org.apache.iceberg_iceberg-spark3-runtime-0.12.1.jar,/root/.ivy2/jars/software.amazon.awssdk_bundle-2.15.40.jar,/root/.ivy2/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,/root/.ivy2/jars/software.amazon.eventstream_eventstream-1.0.1.jar,/root/.ivy2/jars/software.amazon.awssdk_utils-2.15.40.jar,/root/.ivy2/jars/software.amazon.awssdk_annotations-2.15.40.jar,/root/.ivy2/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,/root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.2.jar,/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.28.jar,/root/.ivy2/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar'),\n",
       " ('spark.sql.catalog.demo.warehouse', '/home/iceberg/warehouse'),\n",
       " ('spark.files',\n",
       "  'file:///root/.ivy2/jars/org.apache.iceberg_iceberg-spark3-runtime-0.12.1.jar,file:///root/.ivy2/jars/software.amazon.awssdk_bundle-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.eventstream_eventstream-1.0.1.jar,file:///root/.ivy2/jars/software.amazon.awssdk_utils-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_annotations-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.2.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.28.jar,file:///root/.ivy2/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar'),\n",
       " ('spark.app.startTime', '1637105398822'),\n",
       " ('spark.hadoop.fs.s3.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.app.initial.file.urls',\n",
       "  'file:///root/.ivy2/jars/org.apache.iceberg_iceberg-spark3-runtime-0.12.1.jar,file:///root/.ivy2/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.eventstream_eventstream-1.0.1.jar,file:///root/.ivy2/jars/software.amazon.awssdk_utils-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_annotations-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.28.jar,file:///root/.ivy2/jars/software.amazon.awssdk_bundle-2.15.40.jar,file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.2.jar'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '40709'),\n",
       " ('spark.sql.catalog.demo.io-impl', 'org.apache.iceberg.hadoop.HadoopFileIO'),\n",
       " ('spark.sql.catalog.demo.catalog-impl',\n",
       "  'org.apache.iceberg.jdbc.JdbcCatalog'),\n",
       " ('spark.sql.catalog.demo.uri',\n",
       "  'jdbc:postgresql://postgres:5432/demo_catalog'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.sql.extensions',\n",
       "  'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
       " ('spark.jars',\n",
       "  'file:///root/.ivy2/jars/org.apache.iceberg_iceberg-spark3-runtime-0.12.1.jar,file:///root/.ivy2/jars/software.amazon.awssdk_bundle-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.eventstream_eventstream-1.0.1.jar,file:///root/.ivy2/jars/software.amazon.awssdk_utils-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_annotations-2.15.40.jar,file:///root/.ivy2/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,file:///root/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.2.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.28.jar,file:///root/.ivy2/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', '113ec5fe70f9'),\n",
       " ('spark.sql.catalog.demo', 'org.apache.iceberg.spark.SparkCatalog'),\n",
       " ('spark.sql.defaultCatalog', 'demo'),\n",
       " ('spark.jars.packages',\n",
       "  'org.apache.iceberg:iceberg-spark3-runtime:0.12.1,software.amazon.awssdk:bundle:2.15.40,software.amazon.awssdk:url-connection-client:2.15.40'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/iceberg/notebooks/spark-warehouse'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c37ca92",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o50.csv.\n: org.apache.iceberg.jdbc.UncheckedSQLException: Failed to connect: jdbc:postgresql://postgres:5432/demo_catalog\n\tat org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:53)\n\tat org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:31)\n\tat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:118)\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:49)\n\tat org.apache.iceberg.jdbc.JdbcCatalog.initializeCatalogTables(JdbcCatalog.java:108)\n\tat org.apache.iceberg.jdbc.JdbcCatalog.initialize(JdbcCatalog.java:93)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:193)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:225)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:105)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:388)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:61)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:52)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:117)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$currentNamespace$1(CatalogManager.scala:93)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)\n\tat org.apache.spark.sql.catalyst.optimizer.GetCurrentDatabaseAndCatalog.apply(finishAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.optimizer.GetCurrentDatabaseAndCatalog.apply(finishAnalysis.scala:99)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:87)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:95)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:113)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$simpleString$2(QueryExecution.scala:161)\n\tat org.apache.spark.sql.execution.ExplainUtils$.processPlan(ExplainUtils.scala:115)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:161)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:206)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:175)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:112)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:65)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:209)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:206)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:795)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user \"admin\"\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.doAuthentication(ConnectionFactoryImpl.java:613)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:161)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:213)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:51)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:225)\n\tat org.postgresql.Driver.makeConnection(Driver.java:465)\n\tat org.postgresql.Driver.connect(Driver.java:264)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:51)\n\t... 76 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_212/3987357872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/iceberg/data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o50.csv.\n: org.apache.iceberg.jdbc.UncheckedSQLException: Failed to connect: jdbc:postgresql://postgres:5432/demo_catalog\n\tat org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:53)\n\tat org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:31)\n\tat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:118)\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:49)\n\tat org.apache.iceberg.jdbc.JdbcCatalog.initializeCatalogTables(JdbcCatalog.java:108)\n\tat org.apache.iceberg.jdbc.JdbcCatalog.initialize(JdbcCatalog.java:93)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:193)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:225)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:105)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:388)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:61)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:52)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:117)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$currentNamespace$1(CatalogManager.scala:93)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)\n\tat org.apache.spark.sql.catalyst.optimizer.GetCurrentDatabaseAndCatalog.apply(finishAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.optimizer.GetCurrentDatabaseAndCatalog.apply(finishAnalysis.scala:99)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:87)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:95)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:113)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$simpleString$2(QueryExecution.scala:161)\n\tat org.apache.spark.sql.execution.ExplainUtils$.processPlan(ExplainUtils.scala:115)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:161)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:206)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:175)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:112)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:65)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:209)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:206)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:795)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user \"admin\"\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.doAuthentication(ConnectionFactoryImpl.java:613)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:161)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:213)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:51)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:225)\n\tat org.postgresql.Driver.makeConnection(Driver.java:465)\n\tat org.postgresql.Driver.connect(Driver.java:264)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:51)\n\t... 76 more\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"/home/iceberg/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbfbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"taxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1387d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"insert overwrite taxis select * from taxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from taxis.history\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
