{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1041ae6f",
   "metadata": {},
   "source": [
    "![iceberg-logo](https://www.apache.org/logos/res/iceberg/iceberg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fb2ab",
   "metadata": {},
   "source": [
    "# Getting Started with Iceberg and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5c8206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a1df798ff776:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5ba6e6ec10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a9f41",
   "metadata": {},
   "source": [
    "## Load One Month of NYC Taxi/Limousine Trip Data\n",
    "\n",
    "For this notebook, we will use the New York City Taxi and Limousine Commision Trip Record Data that's available on the AWS Open Data Registry. This contains data of trips taken by taxis and for-hire vehicles in New York City. We'll save this into an iceberg table called `taxis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c37ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|            VendorID|   string|       |\n",
      "|tpep_pickup_datetime|   string|       |\n",
      "|tpep_dropoff_date...|   string|       |\n",
      "|     passenger_count|   string|       |\n",
      "|       trip_distance|   string|       |\n",
      "|          RatecodeID|   string|       |\n",
      "|  store_and_fwd_flag|   string|       |\n",
      "|        PULocationID|   string|       |\n",
      "|        DOLocationID|   string|       |\n",
      "|        payment_type|   string|       |\n",
      "|         fare_amount|   string|       |\n",
      "|               extra|   string|       |\n",
      "|             mta_tax|   string|       |\n",
      "|          tip_amount|   string|       |\n",
      "|        tolls_amount|   string|       |\n",
      "|improvement_surch...|   string|       |\n",
      "|        total_amount|   string|       |\n",
      "|congestion_surcharge|   string|       |\n",
      "|                    |         |       |\n",
      "|      # Partitioning|         |       |\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"/home/iceberg/data/yellow_tripdata_2020-04.csv\")\n",
    "df.write.saveAsTable(\"taxis\")\n",
    "spark.sql(\"DESCRIBE TABLE taxis\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd2c03",
   "metadata": {},
   "source": [
    "## Schema Evolution\n",
    "\n",
    "Adding, dropping, renaming, or altering columns is easy and safe in Iceberg. In this example, we'll rename `fare_amount` to `fare` and `trip_distance` to `distance`. We'll also add a float column `fare_per_distance_unit` immediately after `distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25b8413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE taxis\n",
    "    RENAME COLUMN fare_amount TO fare\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE taxis\n",
    "    RENAME COLUMN trip_distance TO distance\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE taxis\n",
    "    ADD COLUMN fare_per_distance_unit float AFTER distance\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416b498",
   "metadata": {},
   "source": [
    "Let's update the new `fare_per_distance_unit` to equal `fare` divided by `distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ad963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+-----+--------+----------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime| fare|distance|fare_per_distance_unit|\n",
      "+--------+--------------------+---------------------+-----+--------+----------------------+\n",
      "|       1| 2020-04-01 00:41:22|  2020-04-01 01:01:53|  5.5|    1.20|             4.5833335|\n",
      "|       1| 2020-04-01 00:56:00|  2020-04-01 01:09:25| 12.5|    3.40|             3.6764705|\n",
      "|       1| 2020-04-01 00:00:26|  2020-04-01 00:09:25|   10|    2.80|             3.5714285|\n",
      "|       1| 2020-04-01 00:24:38|  2020-04-01 00:34:38|   10|    2.60|             3.8461537|\n",
      "|       2| 2020-04-01 00:13:24|  2020-04-01 00:18:26|  6.5|    1.44|              4.513889|\n",
      "|       2| 2020-04-01 00:24:36|  2020-04-01 00:33:09| 10.5|    2.93|             3.5836177|\n",
      "|       2| 2020-04-01 00:56:56|  2020-04-01 01:09:13|   20|    6.86|              2.915452|\n",
      "|       2| 2020-04-01 00:06:56|  2020-04-01 00:14:15|    7|    1.19|              5.882353|\n",
      "|       1| 2020-04-01 00:50:05|  2020-04-01 01:08:54| 31.5|   11.30|             2.7876105|\n",
      "|       2| 2020-04-01 00:07:10|  2020-04-01 00:18:45|   13|    3.68|             3.5326087|\n",
      "|       1| 2020-04-01 00:37:21|  2020-04-01 00:46:00|    8|    1.40|              5.714286|\n",
      "|       2| 2020-04-01 00:19:44|  2020-04-01 00:22:27|    4|     .55|             7.2727275|\n",
      "|       2| 2020-04-01 00:13:15|  2020-04-01 00:18:30|    6|     .85|             7.0588236|\n",
      "|       2| 2020-04-01 00:34:54|  2020-04-01 01:04:55| 24.5|    7.32|             3.3469946|\n",
      "|       2| 2020-04-01 00:03:53|  2020-04-01 00:12:10|-14.5|    4.37|            -3.3180778|\n",
      "|       2| 2020-04-01 00:03:53|  2020-04-01 00:12:10| 14.5|    4.37|             3.3180778|\n",
      "|       2| 2020-04-01 00:26:37|  2020-04-01 00:37:13|-13.5|    4.55|             -2.967033|\n",
      "|       2| 2020-04-01 00:26:37|  2020-04-01 00:37:13| 13.5|    4.55|              2.967033|\n",
      "|       1| 2020-04-01 00:20:11|  2020-04-01 00:45:10| 29.5|    9.90|              2.979798|\n",
      "|       1| 2020-04-01 00:27:35|  2020-04-01 00:38:50| 12.5|    3.70|             3.3783784|\n",
      "+--------+--------------------+---------------------+-----+--------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "UPDATE taxis\n",
    "SET fare_per_distance_unit = fare/distance\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    VendorID\n",
    "    ,tpep_pickup_datetime\n",
    "    ,tpep_dropoff_datetime\n",
    "    ,fare\n",
    "    ,distance\n",
    "    ,fare_per_distance_unit\n",
    "    FROM taxis\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37582e02",
   "metadata": {},
   "source": [
    "## Row Level Deletes\n",
    "With Iceberg tables, `DELETE` queries can be used to perform row-level deletes. This is as simple as providing the table name and a `WHERE` predicate. If the filter matches an entire partition of the table, Iceberg will intelligently perform a metadata-only operation where it simply deletes the metadata for that partition.\n",
    "\n",
    "Let's perform a row-level delete for all rows that have a `fare_per_distance_unit` greater than 4 and a `distance` greater than 2. This should leave us with relatively short trips that have a relatively high fare per distance traveled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded820f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/19 11:49:08 WARN RewriteDelete: Cannot translate expression to source filter: ((fare_per_distance_unit#304 > 4.0) OR (cast(distance#303 as double) > 2.0))\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    DELETE FROM taxis\n",
    "    WHERE fare_per_distance_unit > 4.0\n",
    "    OR distance > 2.0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef3712",
   "metadata": {},
   "source": [
    "There are some fares that have a `null` for `fare_per_distance_unit` due to the distance being `0`. Let's remove those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b69265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    DELETE FROM taxis\n",
    "    WHERE fare_per_distance_unit is null\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b92d7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+----+--------+----------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|fare|distance|fare_per_distance_unit|\n",
      "+--------+--------------------+---------------------+----+--------+----------------------+\n",
      "|       2| 2020-04-01 00:42:56|  2020-04-01 00:48:41|  -7|    1.34|             -5.223881|\n",
      "|       2| 2020-04-01 00:26:49|  2020-04-01 00:31:04| 6.5|    1.69|             3.8461537|\n",
      "|       2| 2020-04-01 00:42:30|  2020-04-01 00:46:58| 6.5|    1.70|             3.8235295|\n",
      "|       1| 2020-04-01 00:36:15|  2020-04-01 00:41:30|   7|    1.80|             3.8888888|\n",
      "|       2| 2020-04-01 01:16:34|  2020-04-01 01:23:20|  -8|    1.85|             -4.324324|\n",
      "|       2| 2020-04-01 01:51:36|  2020-04-01 01:57:59| 7.5|    1.88|             3.9893618|\n",
      "|       1| 2020-04-01 02:13:27|  2020-04-01 02:16:30|   6|    1.50|                   4.0|\n",
      "|       2| 2020-04-01 02:25:38|  2020-04-01 02:26:05|-2.5|     .08|                -31.25|\n",
      "|       2| 2020-04-01 02:29:54|  2020-04-01 02:33:27|   6|    1.52|             3.9473684|\n",
      "|       2| 2020-04-01 02:39:47|  2020-04-01 02:45:27|  -8|    1.94|             -4.123711|\n",
      "|       1| 2020-04-01 04:57:49|  2020-04-01 05:04:35| 7.5|    1.90|             3.9473684|\n",
      "|       2| 2020-04-01 04:51:12|  2020-04-01 04:59:26|-6.5|     .99|            -6.5656567|\n",
      "|       2| 2020-04-01 04:39:28|  2020-04-01 04:42:55|   6|    1.52|             3.9473684|\n",
      "|       2| 2020-04-01 05:36:43|  2020-04-01 05:41:04|   7|    1.85|             3.7837837|\n",
      "|       2| 2020-04-01 05:41:37|  2020-04-01 05:47:20| 6.5|    1.66|             3.9156628|\n",
      "|       1| 2020-04-01 06:34:29|  2020-04-01 06:39:37|   7|    1.80|             3.8888888|\n",
      "|       2| 2020-04-01 06:32:15|  2020-04-01 06:36:59|   7|    1.87|             3.7433155|\n",
      "|       2| 2020-04-01 06:02:27|  2020-04-01 06:05:49| 6.5|    1.71|             3.8011696|\n",
      "|       2| 2020-04-01 06:18:29|  2020-04-01 06:21:47|  -5|     .97|            -5.1546392|\n",
      "|       2| 2020-04-01 06:57:20|  2020-04-01 07:01:18|   7|    1.90|             3.6842105|\n",
      "+--------+--------------------+---------------------+----+--------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    VendorID\n",
    "    ,tpep_pickup_datetime\n",
    "    ,tpep_dropoff_datetime\n",
    "    ,fare\n",
    "    ,distance\n",
    "    ,fare_per_distance_unit\n",
    "    FROM taxis\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce6bb4",
   "metadata": {},
   "source": [
    "## Metadata Tables\n",
    "\n",
    "Iceberg tables contain very rich metadata that can be easily queried. For example, you can retrieve the manifest list for any snapshot, simply by querying the table's `snapshots` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fade1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/19 11:49:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/11/19 11:49:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/11/19 11:49:14 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "21/11/19 11:49:14 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.22.0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|        snapshot_id|       manifest_list|\n",
      "+-------------------+--------------------+\n",
      "|2698491583658191945|/home/iceberg/war...|\n",
      "|6211071195367590713|/home/iceberg/war...|\n",
      "|6548397803384183321|/home/iceberg/war...|\n",
      "|3201604413922280521|/home/iceberg/war...|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/19 11:49:14 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT snapshot_id, manifest_list\n",
    "    FROM taxis.snapshots\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64887133",
   "metadata": {},
   "source": [
    "The `files` table contains loads of information on data files, including column level statistics such as null counts, lower bounds, and upper bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb712f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+--------------------+--------------------+\n",
      "|           file_path|file_format|record_count|   null_value_counts|        lower_bounds|        upper_bounds|\n",
      "+--------------------+-----------+------------+--------------------+--------------------+--------------------+\n",
      "|/home/iceberg/war...|    PARQUET|        4953|{1 -> 53, 2 -> 0,...|{1 -> 1, 2 -> 202...|{1 -> 2, 2 -> 202...|\n",
      "+--------------------+-----------+------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT file_path, file_format, record_count, null_value_counts, lower_bounds, upper_bounds\n",
    "    FROM taxis.files\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65deb074",
   "metadata": {},
   "source": [
    "## Time Travel\n",
    "\n",
    "The history table lists all snapshots and which parent snapshot they derive from. The `is_current_ancestor` flag let's you know if a snapshot is part of the linear history of the current snapshot of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab64f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2021-11-19 11:49:...|2698491583658191945|               null|               true|\n",
      "|2021-11-19 11:49:...|6211071195367590713|2698491583658191945|               true|\n",
      "|2021-11-19 11:49:...|6548397803384183321|6211071195367590713|               true|\n",
      "|2021-11-19 11:49:...|3201604413922280521|6548397803384183321|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM taxis.history\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47129d69",
   "metadata": {},
   "source": [
    "You can time-travel by altering the `current-snapshot-id` property of the table to reference any snapshot in the table's history. Let's revert the table to it's original state by traveling to the very first snapshot ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8df43d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698491583658191945\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM taxis.history\n",
    "\"\"\")\n",
    "original_snapshot = df.head().snapshot_id\n",
    "print(original_snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "955a4c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+--------------------+---------------------+-----+--------+----------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime| fare|distance|fare_per_distance_unit|\n",
      "+--------+--------------------+---------------------+-----+--------+----------------------+\n",
      "|       1| 2020-04-01 00:41:22|  2020-04-01 01:01:53|  5.5|    1.20|                  null|\n",
      "|       1| 2020-04-01 00:56:00|  2020-04-01 01:09:25| 12.5|    3.40|                  null|\n",
      "|       1| 2020-04-01 00:00:26|  2020-04-01 00:09:25|   10|    2.80|                  null|\n",
      "|       1| 2020-04-01 00:24:38|  2020-04-01 00:34:38|   10|    2.60|                  null|\n",
      "|       2| 2020-04-01 00:13:24|  2020-04-01 00:18:26|  6.5|    1.44|                  null|\n",
      "|       2| 2020-04-01 00:24:36|  2020-04-01 00:33:09| 10.5|    2.93|                  null|\n",
      "|       2| 2020-04-01 00:56:56|  2020-04-01 01:09:13|   20|    6.86|                  null|\n",
      "|       2| 2020-04-01 00:06:56|  2020-04-01 00:14:15|    7|    1.19|                  null|\n",
      "|       1| 2020-04-01 00:50:05|  2020-04-01 01:08:54| 31.5|   11.30|                  null|\n",
      "|       2| 2020-04-01 00:07:10|  2020-04-01 00:18:45|   13|    3.68|                  null|\n",
      "|       1| 2020-04-01 00:37:21|  2020-04-01 00:46:00|    8|    1.40|                  null|\n",
      "|       2| 2020-04-01 00:19:44|  2020-04-01 00:22:27|    4|     .55|                  null|\n",
      "|       2| 2020-04-01 00:13:15|  2020-04-01 00:18:30|    6|     .85|                  null|\n",
      "|       2| 2020-04-01 00:34:54|  2020-04-01 01:04:55| 24.5|    7.32|                  null|\n",
      "|       2| 2020-04-01 00:03:53|  2020-04-01 00:12:10|-14.5|    4.37|                  null|\n",
      "|       2| 2020-04-01 00:03:53|  2020-04-01 00:12:10| 14.5|    4.37|                  null|\n",
      "|       2| 2020-04-01 00:26:37|  2020-04-01 00:37:13|-13.5|    4.55|                  null|\n",
      "|       2| 2020-04-01 00:26:37|  2020-04-01 00:37:13| 13.5|    4.55|                  null|\n",
      "|       1| 2020-04-01 00:20:11|  2020-04-01 00:45:10| 29.5|    9.90|                  null|\n",
      "|       1| 2020-04-01 00:27:35|  2020-04-01 00:38:50| 12.5|    3.70|                  null|\n",
      "+--------+--------------------+---------------------+-----+--------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE taxis\n",
    "    SET TBLPROPERTIES ('current-snapshot-id'={original_snapshot})\n",
    "\"\"\").show()\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    VendorID\n",
    "    ,tpep_pickup_datetime\n",
    "    ,tpep_dropoff_datetime\n",
    "    ,fare\n",
    "    ,distance\n",
    "    ,fare_per_distance_unit\n",
    "    FROM taxis\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b71c76",
   "metadata": {},
   "source": [
    "Another look at the history table shows that the original state of the table has been added as a new entry\n",
    "with the original snapshot ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91b801d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2021-11-19 11:49:...|2698491583658191945|               null|               true|\n",
      "|2021-11-19 11:49:...|6211071195367590713|2698491583658191945|              false|\n",
      "|2021-11-19 11:49:...|6548397803384183321|6211071195367590713|              false|\n",
      "|2021-11-19 11:49:...|3201604413922280521|6548397803384183321|              false|\n",
      "|2021-11-19 11:49:...|2698491583658191945|               null|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM taxis.history\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85667efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+-----+--------+----------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime| fare|distance|fare_per_distance_unit|\n",
      "+--------+--------------------+---------------------+-----+--------+----------------------+\n",
      "|       1| 2020-04-01 00:41:22|  2020-04-01 01:01:53|  5.5|    1.20|                  null|\n",
      "|       1| 2020-04-01 00:56:00|  2020-04-01 01:09:25| 12.5|    3.40|                  null|\n",
      "|       1| 2020-04-01 00:00:26|  2020-04-01 00:09:25|   10|    2.80|                  null|\n",
      "|       1| 2020-04-01 00:24:38|  2020-04-01 00:34:38|   10|    2.60|                  null|\n",
      "|       2| 2020-04-01 00:13:24|  2020-04-01 00:18:26|  6.5|    1.44|                  null|\n",
      "|       2| 2020-04-01 00:24:36|  2020-04-01 00:33:09| 10.5|    2.93|                  null|\n",
      "|       2| 2020-04-01 00:56:56|  2020-04-01 01:09:13|   20|    6.86|                  null|\n",
      "|       2| 2020-04-01 00:06:56|  2020-04-01 00:14:15|    7|    1.19|                  null|\n",
      "|       1| 2020-04-01 00:50:05|  2020-04-01 01:08:54| 31.5|   11.30|                  null|\n",
      "|       2| 2020-04-01 00:07:10|  2020-04-01 00:18:45|   13|    3.68|                  null|\n",
      "|       1| 2020-04-01 00:37:21|  2020-04-01 00:46:00|    8|    1.40|                  null|\n",
      "|       2| 2020-04-01 00:19:44|  2020-04-01 00:22:27|    4|     .55|                  null|\n",
      "|       2| 2020-04-01 00:13:15|  2020-04-01 00:18:30|    6|     .85|                  null|\n",
      "|       2| 2020-04-01 00:34:54|  2020-04-01 01:04:55| 24.5|    7.32|                  null|\n",
      "|       2| 2020-04-01 00:03:53|  2020-04-01 00:12:10|-14.5|    4.37|                  null|\n",
      "|       2| 2020-04-01 00:03:53|  2020-04-01 00:12:10| 14.5|    4.37|                  null|\n",
      "|       2| 2020-04-01 00:26:37|  2020-04-01 00:37:13|-13.5|    4.55|                  null|\n",
      "|       2| 2020-04-01 00:26:37|  2020-04-01 00:37:13| 13.5|    4.55|                  null|\n",
      "|       1| 2020-04-01 00:20:11|  2020-04-01 00:45:10| 29.5|    9.90|                  null|\n",
      "|       1| 2020-04-01 00:27:35|  2020-04-01 00:38:50| 12.5|    3.70|                  null|\n",
      "+--------+--------------------+---------------------+-----+--------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    VendorID\n",
    "    ,tpep_pickup_datetime\n",
    "    ,tpep_dropoff_datetime\n",
    "    ,fare\n",
    "    ,distance\n",
    "    ,fare_per_distance_unit\n",
    "    FROM taxis\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
